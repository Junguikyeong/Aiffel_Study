{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "distributed-match",
   "metadata": {},
   "source": [
    "# 1. Data Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "accessory-fifteen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [' There must be some kind of way outta here', 'Said the joker to the thief', \"There's too much confusion\", \"I can't get no relief Business men, they drink my wine\", 'Plowman dig my earth', 'None were level on the mind', 'Nobody up at his word', 'Hey, hey No reason to get excited', 'The thief he kindly spoke', 'There are many here among us', 'Who feel that life is but a joke', \"But, uh, but you and I, we've been through that\", 'And this is not our fate', \"So let us stop talkin' falsely now\", \"The hour's getting late, hey All along the watchtower\", 'Princes kept the view', 'While all the women came and went', 'Barefoot servants, too', 'Outside in the cold distance', 'A wildcat did growl']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-change",
   "metadata": {},
   "source": [
    "# 2. Data purification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bacterial-volunteer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> there must be some kind of way outta here <end>',\n",
       " '<start> said the joker to the thief <end>',\n",
       " '<start> there s too much confusion <end>',\n",
       " '<start> i can t get no relief business men , they drink my wine <end>',\n",
       " '<start> plowman dig my earth <end>',\n",
       " '<start> none were level on the mind <end>',\n",
       " '<start> nobody up at his word <end>',\n",
       " '<start> hey , hey no reason to get excited <end>',\n",
       " '<start> the thief he kindly spoke <end>',\n",
       " '<start> there are many here among us <end>',\n",
       " '<start> who feel that life is but a joke <end>',\n",
       " '<start> and this is not our fate <end>',\n",
       " '<start> so let us stop talkin falsely now <end>',\n",
       " '<start> the hour s getting late , hey all along the watchtower <end>',\n",
       " '<start> princes kept the view <end>',\n",
       " '<start> while all the women came and went <end>',\n",
       " '<start> barefoot servants , too <end>',\n",
       " '<start> outside in the cold distance <end>',\n",
       " '<start> a wildcat did growl <end>',\n",
       " '<start> two riders were approaching <end>']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    if len(sentence.split()) > 15: \n",
    "        return 0\n",
    "    return sentence\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if preprocessed_sentence == 0:\n",
    "        continue\n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-carbon",
   "metadata": {},
   "source": [
    "## 2) Tokenizer 패키지로 텐서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "alleged-sailing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  62 271 ...   0   0   0]\n",
      " [  2 118   6 ...   0   0   0]\n",
      " [  2  62  17 ...   0   0   0]\n",
      " ...\n",
      " [  2  75  45 ...   3   0   0]\n",
      " [  2  49   5 ...   0   0   0]\n",
      " [  2  13 633 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f12ff020ed0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,   # 전체 단어의 개수 \n",
    "        filters=' ',       # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # padding으로 입력 데이터의 시퀀스 길이를 일정하게 맞추기 (maxlen 15로 설정) \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=15)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "metallic-advancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   62  271   27   94  546   20   86  742   90    3    0    0    0\n",
      "     0]\n",
      " [   2  118    6 6263   10    6 2307    3    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2   62   17  102  184 2711    3    0    0    0    0    0    0    0\n",
      "     0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(156013, 15)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변환된 텐서 확인\n",
    "print(tensor[:3])\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "equivalent-nudist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# 단어사전 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "innovative-library",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  62 271  27  94 546  20  86 742  90   3   0   0   0]\n",
      "[ 62 271  27  94 546  20  86 742  90   3   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-precipitation",
   "metadata": {},
   "source": [
    "# 3. Separate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "subtle-cotton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tensor를 train, test 데이터로 분리\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "consistent-incidence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "foster-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-evanescence",
   "metadata": {},
   "source": [
    "# 4. Model design and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "permanent-brooks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-3.77245247e-04, -8.58035264e-07, -1.54673573e-04, ...,\n",
       "         -2.36285967e-04,  3.21980442e-05, -1.63615856e-04],\n",
       "        [-3.85353400e-04,  1.29268476e-04, -3.38507321e-04, ...,\n",
       "         -1.65302074e-04,  9.29189264e-05, -2.59378314e-04],\n",
       "        [-4.14758892e-04,  4.37662937e-04, -3.36711964e-04, ...,\n",
       "         -7.84124204e-05,  1.26686007e-06, -4.48922598e-04],\n",
       "        ...,\n",
       "        [-1.17040111e-03,  1.82645128e-03, -1.70480693e-03, ...,\n",
       "          6.64320833e-04, -4.49582119e-04, -2.67643132e-03],\n",
       "        [-1.38644828e-03,  1.92695251e-03, -1.95899373e-03, ...,\n",
       "          7.63081887e-04, -7.67385063e-04, -2.89417873e-03],\n",
       "        [-1.59461761e-03,  2.01172032e-03, -2.19243672e-03, ...,\n",
       "          8.63062218e-04, -1.07874640e-03, -3.08343326e-03]],\n",
       "\n",
       "       [[-3.77245247e-04, -8.58035264e-07, -1.54673573e-04, ...,\n",
       "         -2.36285967e-04,  3.21980442e-05, -1.63615856e-04],\n",
       "        [-7.62600161e-04, -2.05553624e-05, -8.42666122e-05, ...,\n",
       "         -2.02239462e-04,  7.43328783e-05, -3.89217399e-04],\n",
       "        [-1.25825417e-03, -1.04843763e-04,  1.06621104e-04, ...,\n",
       "         -7.48738676e-05,  2.17164838e-04, -3.85790423e-04],\n",
       "        ...,\n",
       "        [-7.23908714e-04, -1.20297285e-04,  2.77467974e-04, ...,\n",
       "          3.61716411e-05,  5.11206512e-04, -1.31718291e-03],\n",
       "        [-8.12225335e-04,  1.78075439e-04, -1.10990841e-04, ...,\n",
       "          1.43202269e-04,  2.96361861e-04, -1.58138166e-03],\n",
       "        [-9.50678193e-04,  4.83873184e-04, -5.08996251e-04, ...,\n",
       "          2.55269959e-04,  9.74286741e-06, -1.86067575e-03]],\n",
       "\n",
       "       [[-3.77245247e-04, -8.58035264e-07, -1.54673573e-04, ...,\n",
       "         -2.36285967e-04,  3.21980442e-05, -1.63615856e-04],\n",
       "        [-5.07429068e-04,  7.06094870e-05, -2.47421965e-04, ...,\n",
       "         -3.54939344e-04,  6.37655512e-06, -2.46852956e-04],\n",
       "        [-5.09565405e-04,  2.54768267e-04, -3.85163701e-04, ...,\n",
       "         -1.79499475e-04,  2.05301942e-04, -2.65339157e-04],\n",
       "        ...,\n",
       "        [-1.30408898e-03, -1.80188872e-04,  4.06504878e-05, ...,\n",
       "          1.12392905e-03, -8.36014166e-04,  4.55771253e-04],\n",
       "        [-1.42679131e-03,  5.16637556e-06, -2.14841377e-04, ...,\n",
       "          1.19595556e-03, -8.22277856e-04,  5.15567335e-05],\n",
       "        [-1.56126847e-03,  2.51908292e-04, -5.11047081e-04, ...,\n",
       "          1.22409977e-03, -8.55880149e-04, -3.87061998e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.77245247e-04, -8.58035264e-07, -1.54673573e-04, ...,\n",
       "         -2.36285967e-04,  3.21980442e-05, -1.63615856e-04],\n",
       "        [-5.07429068e-04,  7.06094870e-05, -2.47421965e-04, ...,\n",
       "         -3.54939344e-04,  6.37655512e-06, -2.46852956e-04],\n",
       "        [-6.51792332e-04,  1.54882524e-04, -4.33734269e-04, ...,\n",
       "         -5.45755029e-04, -1.73037697e-04, -5.64351387e-04],\n",
       "        ...,\n",
       "        [-5.53489022e-04,  3.58453981e-04, -5.11067046e-04, ...,\n",
       "          8.31837533e-04,  6.13794604e-04, -1.25185272e-03],\n",
       "        [-7.25999416e-04,  5.62223664e-04, -6.44140644e-04, ...,\n",
       "          9.43252409e-04,  5.02071052e-04, -1.32930372e-03],\n",
       "        [-9.12676856e-04,  8.14112311e-04, -8.31765181e-04, ...,\n",
       "          1.02998968e-03,  3.12495366e-04, -1.45146437e-03]],\n",
       "\n",
       "       [[-3.77245247e-04, -8.58035264e-07, -1.54673573e-04, ...,\n",
       "         -2.36285967e-04,  3.21980442e-05, -1.63615856e-04],\n",
       "        [-5.52498910e-04,  4.33038076e-05, -3.02842207e-04, ...,\n",
       "         -5.02507552e-04,  5.31682235e-05, -3.83694220e-04],\n",
       "        [-4.99607995e-04, -3.97014483e-05, -3.76409822e-04, ...,\n",
       "         -6.48927467e-04,  1.50272725e-04, -2.57069536e-04],\n",
       "        ...,\n",
       "        [ 3.56315490e-04, -2.22497969e-04, -8.85485439e-04, ...,\n",
       "          5.85512549e-04, -1.60322277e-04, -1.21190690e-03],\n",
       "        [ 1.81536467e-04,  1.25923310e-04, -1.06152159e-03, ...,\n",
       "          6.42234692e-04, -2.44708179e-04, -1.40356086e-03],\n",
       "        [-5.85738962e-05,  4.78196482e-04, -1.26961072e-03, ...,\n",
       "          6.93235954e-04, -3.94490868e-04, -1.61071855e-03]],\n",
       "\n",
       "       [[-3.77245247e-04, -8.58035264e-07, -1.54673573e-04, ...,\n",
       "         -2.36285967e-04,  3.21980442e-05, -1.63615856e-04],\n",
       "        [-5.07429068e-04,  7.06094870e-05, -2.47421965e-04, ...,\n",
       "         -3.54939344e-04,  6.37655512e-06, -2.46852956e-04],\n",
       "        [-5.09565405e-04,  2.54768267e-04, -3.85163701e-04, ...,\n",
       "         -1.79499475e-04,  2.05301942e-04, -2.65339157e-04],\n",
       "        ...,\n",
       "        [ 6.60845952e-04, -3.80258076e-04,  8.18168861e-04, ...,\n",
       "         -3.97356838e-04, -5.94483980e-04,  3.46291563e-05],\n",
       "        [ 6.65611355e-04, -2.10749437e-04,  5.53380523e-04, ...,\n",
       "         -4.23638703e-04, -7.17121118e-04, -1.48939493e-04],\n",
       "        [ 6.00404164e-04, -1.26625600e-04,  1.30373825e-04, ...,\n",
       "         -3.14696576e-04, -5.66122821e-04, -3.76814045e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "stuck-sugar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-burlington",
   "metadata": {},
   "source": [
    "# 4. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "royal-sleeve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "609/609 [==============================] - 199s 323ms/step - loss: 3.9136\n",
      "Epoch 2/10\n",
      "609/609 [==============================] - 198s 325ms/step - loss: 3.0097\n",
      "Epoch 3/10\n",
      "609/609 [==============================] - 198s 325ms/step - loss: 2.8180\n",
      "Epoch 4/10\n",
      "609/609 [==============================] - 198s 324ms/step - loss: 2.6731\n",
      "Epoch 5/10\n",
      "609/609 [==============================] - 198s 324ms/step - loss: 2.5588\n",
      "Epoch 6/10\n",
      "609/609 [==============================] - 198s 324ms/step - loss: 2.4526\n",
      "Epoch 7/10\n",
      "609/609 [==============================] - 197s 323ms/step - loss: 2.3601\n",
      "Epoch 8/10\n",
      "609/609 [==============================] - 197s 324ms/step - loss: 2.2653\n",
      "Epoch 9/10\n",
      "609/609 [==============================] - 197s 323ms/step - loss: 2.1850\n",
      "Epoch 10/10\n",
      "609/609 [==============================] - 197s 323ms/step - loss: 2.1126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1281850a90>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer와 loss등은 차차 배웁니다\n",
    "# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "defined-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "disturbed-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 생성 함수 실행하여 모델에게 작문 시켜보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "central-resort",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> <unk> <unk> <unk> <unk> <end> '"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> aiffel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "innovative-company",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> happy birthday , happy birthday , happy birthday woo , shake ! <end> '"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "lesser-allocation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> love is a beautiful thing <end> '"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "violent-livestock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> computer blue <end> '"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "tender-studio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> sky is the limit and you know that you can have <end> '"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> sky\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-contract",
   "metadata": {},
   "source": [
    "---\n",
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-wells",
   "metadata": {},
   "source": [
    "+ 1. AIFFEL같은 대중적이지 않은 단어로는 그럴듯한 문장이 생성되지 않았지만\n",
    "happy, love, sky 등 대중적인 단어로는 학습된 효과로 그럴듯한 문장이 생성되는것을 확이할 수 가 있었다.\n",
    "+ 2. 데이터 정제부분에서 정규표현식을 이용해서 필요없는 부분을 제거하였고 \n",
    "15 개 이상이 안되어야하는 문장제거는 start,end 부분을 포함해서 15개이상을 확인하여야 하기때문에 필요없는 부분을 제거하고 마지막 부분에 len(sentence.split()) > 15:  조건문을 추가하여서  진행하였다.   \n",
    "패딩 같은 경우는 머신러닝에서는 디폴트값도 pre로 되있는데 이번 ex에는 뒤에 0을 추가하는 post 로 되었었다. pre가 더 높은 결과를 보여준다고 되있었는데 이번 모델에서는 별반 차이가 없어서 post로 계속해서 진행을하였다.\n",
    "+ 3. loss값이 점차 잘 감소하는것을 확인하였고, 2.1로 나쁘지 않은 loss 값이 나왔다.\n",
    "\n",
    "+ 4. 학습을 마치고나서 다양한 입력단어를 작문을 시켜보았는데 나쁘지 않았던 것 같았다 더 낮은 loss값을 추출하고 더 생소한 단어들을 적더라도 좋은 작문이 될 수 있게 만들어가는것이 좋을것같다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
